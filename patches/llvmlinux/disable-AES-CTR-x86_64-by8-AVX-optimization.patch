From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date: Thu, 15 Dec 2016 17:17:52 +0100
Subject: [PATCH] {LLVMLinux} disable AES CTR x86_64 "by8" AVX optimization

LLVM does not handle .altmacro well:

    arch/x86/crypto/aes_ctrby8_avx-x86_64.S:552: Error: too many memory
    references for `vaesenc'
    arch/x86/crypto/aes_ctrby8_avx-x86_64.S:552: Error: bad expression
    arch/x86/crypto/aes_ctrby8_avx-x86_64.S:552: Error: junk at end of
    line, first unrecognized character is `x'

cf. http://llvm.linuxfoundation.org/index.php/Broken_kernel_options
and LLVM bug https://llvm.org/bugs/show_bug.cgi?id=18918
---
 arch/x86/crypto/Makefile           |  5 ++++-
 arch/x86/crypto/aesni-intel_glue.c | 12 ++++++------
 2 files changed, 10 insertions(+), 7 deletions(-)

diff --git a/arch/x86/crypto/Makefile b/arch/x86/crypto/Makefile
index 5f07333bb224..5fb491ae4523 100644
--- a/arch/x86/crypto/Makefile
+++ b/arch/x86/crypto/Makefile
@@ -90,7 +90,10 @@ ifeq ($(avx2_supported),yes)
 endif
 
 aesni-intel-y := aesni-intel_asm.o aesni-intel_glue.o fpu.o
-aesni-intel-$(CONFIG_64BIT) += aesni-intel_avx-x86_64.o aes_ctrby8_avx-x86_64.o
+aesni-intel-$(CONFIG_64BIT) += aesni-intel_avx-x86_64.o
+ifneq ($(cc-name),clang)
+	aesni-intel-$(CONFIG_64BIT) += aes_ctrby8_avx-x86_64.o
+endif
 ghash-clmulni-intel-y := ghash-clmulni-intel_asm.o ghash-clmulni-intel_glue.o
 sha1-ssse3-y := sha1_ssse3_asm.o sha1_ssse3_glue.o
 poly1305-x86_64-y := poly1305-sse2-x86_64.o poly1305_glue.o
diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
index 3bf3dcf29825..12827de1606a 100644
--- a/arch/x86/crypto/aesni-intel_glue.c
+++ b/arch/x86/crypto/aesni-intel_glue.c
@@ -143,7 +143,7 @@ asmlinkage void aesni_gcm_dec(void *ctx, u8 *out,
 			u8 *auth_tag, unsigned long auth_tag_len);
 
 
-#ifdef CONFIG_AS_AVX
+#if defined(CONFIG_AS_AVX) && !defined(__clang__)
 asmlinkage void aes_ctr_enc_128_avx_by8(const u8 *in, u8 *iv,
 		void *keys, u8 *out, unsigned int num_bytes);
 asmlinkage void aes_ctr_enc_192_avx_by8(const u8 *in, u8 *iv,
@@ -200,7 +200,7 @@ static void aesni_gcm_dec_avx(void *ctx, u8 *out,
 }
 #endif
 
-#ifdef CONFIG_AS_AVX2
+#if defined(CONFIG_AS_AVX2) && !defined(__clang__)
 /*
  * asmlinkage void aesni_gcm_precomp_avx_gen4()
  * gcm_data *my_ctx_data, context data
@@ -481,7 +481,7 @@ static void ctr_crypt_final(struct crypto_aes_ctx *ctx,
 	crypto_inc(ctrblk, AES_BLOCK_SIZE);
 }
 
-#ifdef CONFIG_AS_AVX
+#if defined(CONFIG_AS_AVX) && !defined(__clang__)
 static void aesni_ctr_enc_avx_tfm(struct crypto_aes_ctx *ctx, u8 *out,
 			      const u8 *in, unsigned int len, u8 *iv)
 {
@@ -1216,14 +1216,14 @@ static int __init aesni_init(void)
 	if (!x86_match_cpu(aesni_cpu_id))
 		return -ENODEV;
 #ifdef CONFIG_X86_64
-#ifdef CONFIG_AS_AVX2
+#if defined(CONFIG_AS_AVX2) && !defined(__clang__)
 	if (boot_cpu_has(X86_FEATURE_AVX2)) {
 		pr_info("AVX2 version of gcm_enc/dec engaged.\n");
 		aesni_gcm_enc_tfm = aesni_gcm_enc_avx2;
 		aesni_gcm_dec_tfm = aesni_gcm_dec_avx2;
 	} else
 #endif
-#ifdef CONFIG_AS_AVX
+#if defined(CONFIG_AS_AVX) && !defined(__clang__)
 	if (boot_cpu_has(X86_FEATURE_AVX)) {
 		pr_info("AVX version of gcm_enc/dec engaged.\n");
 		aesni_gcm_enc_tfm = aesni_gcm_enc_avx;
@@ -1236,7 +1236,7 @@ static int __init aesni_init(void)
 		aesni_gcm_dec_tfm = aesni_gcm_dec;
 	}
 	aesni_ctr_enc_tfm = aesni_ctr_enc;
-#ifdef CONFIG_AS_AVX
+#if defined(CONFIG_AS_AVX) && !defined(__clang__)
 	if (boot_cpu_has(X86_FEATURE_AVX)) {
 		/* optimize performance of ctr mode encryption transform */
 		aesni_ctr_enc_tfm = aesni_ctr_enc_avx_tfm;
-- 
